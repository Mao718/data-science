{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import tree\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import f1_score\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_selection import VarianceThreshold\nimport category_encoders as ce\n\nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2023-09-13T05:27:41.502365Z","iopub.execute_input":"2023-09-13T05:27:41.503318Z","iopub.status.idle":"2023-09-13T05:27:45.729678Z","shell.execute_reply.started":"2023-09-13T05:27:41.503279Z","shell.execute_reply":"2023-09-13T05:27:45.728326Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Load and merge the data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"train.csv\").set_index('index')\ntest = pd.read_csv(\"test.csv\").set_index('index')\n\ntrain_ind = train.index\ntest_ind = test.index\n\n\ncard = pd.read_csv(\"card.csv\")\nuser = pd.read_csv(\"user.csv\")\n\n## drop unuse train col\n# train_col = train.columns\n# train_col =train_col.drop('index' )\n# train_col =train_col.drop('card_id' )\n\n## -merge user\ntrain = train.merge(user, how='left')\ntest = test.merge(user, how='left')\n\n# # ## --merge card\n# train = train.merge(card)\n# test = test.merge(card)\n\ntrain = train.set_index(train_ind)\ntest = test.set_index(test_ind)","metadata":{},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessor","metadata":{}},{"cell_type":"code","source":"class preprocessor:\n    def __init__(self ):\n#         self.train_col = train_col\n#         self.test_col = self.train_col.drop('is_fraud?')\n        a =1\n    #def fit(self, data):\n        self.cate = ['']\n        self.target = 'is_fraud?'\n        self.target_emb_feature = ['merchant_city']\n        self.drop_user_info = ['birth_year' , 'birth_month' ]\n        self.drop_card_info = [ 'expires' ,'acct_open_date' ,'card_brand' , 'card_type'  , 'has_chip' ]\n        self.user_obj_to_cat = ['user_id','gender' , 'address','city' ,'state' , 'zipcode' ]\n        self.card_obj_to_cat =  [ ]\n        \n        self.user_emb_feature = []#  ,'mcc','merchant_id' 'merchant_city' , 'merchant_state', 'merchant_state'\n        self.money_col = ['amount','per_capita_income_zipcode' ,'yearly_income_person','total_debt'] #,'credit_limit'\n        self.date_col = []\n        self.GLMM = {}\n        \n    \n        \n    def common_transform(self, data):\n        x = data.copy()\n        x = x.fillna(0)\n        ### ---------- chage to category\n        for feat in self.user_obj_to_cat:\n            x[feat] = x[feat].astype('category')\n        for feat in self.card_obj_to_cat:\n            x[feat] = x[feat].astype('category')\n        x['zip'] = x['zip'].astype('category')\n        x['mcc'] = x['mcc'].astype('category')\n        #### drop features---------------\n        x = x.drop(columns = 'card_id')\n        \n        x = x.drop(columns = self.drop_user_info)\n#         x = x.drop(columns = self.drop_card_info)\n        ## one hot errors\n        x['errors?'] = x['errors?'].replace('OK',1).replace('ERROR',0)\n        x['errors?'] = x['errors?'].astype(int) \n        #x['has_chip'] = x['has_chip'].replace('YES',1).replace('NO',0)\n        ### remove $\n        \n        for mc in self.money_col:\n            x[mc] = x[mc].replace('[\\$,]', '', regex=True).astype(float)\n                ## split the date\n        for col in self.date_col:\n            temp = all_data[col].str.split(\"/\",n=1,expand = True)\n            x[col+'_month'] = temp[0]\n            x[col+'_year'] = temp[1]\n            x = x.drop(columns= col)\n            x[col+'_month'] = x[col+'_month'].astype(int)\n            x[col+'_year'] = x[col+'_year'].astype(int)\n        for col in x.select_dtypes(object).columns:\n            x[col] = x[col].astype('category')\n        \n        \n        \n        return x\n    def GLMM_fit(self , x , encode_feature , y):\n        \n        self.GLMM[encode_feature] = ce.glmm.GLMMEncoder()\n        self.GLMM[encode_feature].fit(x[encode_feature] , y )\n        \n        \n    def GLMM_encode(self , x , encode_feature):\n        return  self.GLMM[encode_feature].transform(x[encode_feature])\n    \n    def fit_transform(self, data):\n        \n        x = self.common_transform(data)\n        x['mcc_fre'] = 0\n        self.user_mcc_freq = {}\n        for user in x[\"user_id\"].unique():\n            user_fre = {}\n            for mcc in x['mcc'].unique():\n                user_fre[mcc] = (x[x['user_id'] == user][x['mcc']==mcc] ).shape[0]  /x[ x[\"user_id\"] ==user].shape[0] \n            self.user_mcc_freq[user] = user_fre\n        self.user_mcc_freq = pd.DataFrame.from_dict( self.user_mcc_freq)\n        \n        \n        \n        for user in x[\"user_id\"].unique():\n            user_fre = {}\n            for mcc in x['mcc'].unique():\n                x['mcc_fre'] = np.where( np.logical_and(x['user_id'] == user , x['mcc']==mcc) , self.user_mcc_freq.loc[mcc , user]  ,x['mcc_fre'])\n                #x.loc[x['user_id'] == user][x['mcc']==mcc]['mcc_fre'] = self.user_mcc_freq.loc[mcc , user] \n        \n        \n        self.GLMM_fit(x , \"user_id\" , x['amount'])\n        x[\"glmm_amount_user_id\"] = self.GLMM_encode(x , \"user_id\")\n        \n        cate_feature = x.dtypes\n        self.cate_feature = list(cate_feature[cate_feature == \"category\"].index)\n        self.label_emb = {}\n        \n        \n        \n        \n        for feature in self.cate_feature:\n            self.label_emb[feature] = x[[feature, 'is_fraud?']].groupby(feature).mean()['is_fraud?']\n        \n        self.price_avg = x[['amount','user_id']].groupby('user_id').mean()['amount']\n        \n        x['price_avg'] = x['user_id'].copy()\n        x['price_avg'] = self.price_avg.loc[x['price_avg']].values \n        \n        for feature in self.cate_feature:\n            x[feature] = self.label_emb[feature].loc[x[feature]].values \n        for feat__ in self.user_emb_feature:\n            self.GLMM_fit(x , \"user_id\" ,x[feat__] )\n            x[feat__] = self.GLMM_encode(x , \"user_id\")\n        ## average spend\n        \n        \n        return x\n    def expand(self, feature_mapping, test_data):\n        return pd.DataFrame([test_data.value_counts(), feature_mapping], index = [\"test\", \"train\"]).T[\"train\"]\n    def transform(self, data):\n        x = self.common_transform(data)\n        x['mcc_fre'] = 0\n        for user in x[\"user_id\"].unique():\n            user_fre = {}\n            for mcc in x['mcc'].unique():\n                x['mcc_fre'] = np.where( np.logical_and(x['user_id'] == user , x['mcc']==mcc) , self.user_mcc_freq.loc[mcc , user]  ,x['mcc_fre'])\n                \n        x[\"glmm_amount_user_id\"] = self.GLMM_encode(x , \"user_id\")\n        ## ---- price_avg\n        x['price_avg'] = x['user_id'].copy()\n        x['price_avg'] = self.price_avg.loc[x['price_avg']].values \n        \n        for feature in self.cate_feature:\n            self.label_emb[feature] = self.expand(self.label_emb[feature], x[feature])\n            x[feature] = self.label_emb[feature].loc[x[feature]].values\n        for feat__ in self.user_emb_feature:\n            x[feat__] = self.GLMM_encode(x , \"user_id\")\n        return x","metadata":{},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","metadata":{},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"code","source":"pre = preprocessor()\ntrain_pre = pre.fit_transform(train)\nx , y = train_pre.drop(columns = ['is_fraud?']) ,train_pre['is_fraud?']\n\n\ntest_pre = pre.transform(test)\n","metadata":{},"execution_count":5,"outputs":[{"name":"stderr","output_type":"stream","text":"C:\\Users\\zxc59\\AppData\\Local\\Temp\\ipykernel_21188\\1411446235.py:76: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n\n  user_fre[mcc] = (x[x['user_id'] == user][x['mcc']==mcc] ).shape[0]  /x[ x[\"user_id\"] ==user].shape[0]\n"}]},{"cell_type":"markdown","source":"# Setting up grid search parameters","metadata":{}},{"cell_type":"code","source":"params = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n    \n        'max_depth': [4, 5, 6 , 7 ,8, 10],\n        \"scale_pos_weight\" : [2,5,8,10] , \n        'n_estimators' : [512,1024]\n        }","metadata":{},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom datetime import datetime\nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 1001)\n\nxgb = XGBClassifier( objective='binary:logistic')\ngrid = GridSearchCV(estimator=xgb, param_grid=params, scoring='f1', n_jobs=4, cv=skf.split(x,y), verbose=3 )\n\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\ngrid.fit(x, y)\ntimer(start_time)","metadata":{},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":"Fitting 5 folds for each of 3 candidates, totalling 15 fits\n\n\n\n Time taken: 0 hours 1 minutes and 1.3 seconds.\n"}]},{"cell_type":"code","source":"print('\\n All results:')\nprint(grid.cv_results_)\nprint('\\n Best estimator:')\nprint(grid.best_estimator_)\nprint('\\n Best score:')\nprint(grid.best_score_ * 2 - 1)\nprint('\\n Best parameters:')\nprint(grid.best_params_)\nresults = pd.DataFrame(grid.cv_results_)\nresults.to_csv('xgb-grid-search-results-01.csv', index=False)\n\n# y_test = grid.best_estimator_.predict_proba(test)\n# results_df = pd.DataFrame(data={'id':test_df['id'], 'target':y_test[:,1]})\n# results_df.to_csv('submission-grid-search-xgb-porto-01.csv', index=False)","metadata":{"scrolled":true},"execution_count":8,"outputs":[{"name":"stdout","output_type":"stream","text":"\n\n All results:\n\n{'mean_fit_time': array([13.54119697, 13.45896134, 12.29039888]), 'std_fit_time': array([0.07398839, 0.05052266, 0.94206049]), 'mean_score_time': array([0.0666008 , 0.06427259, 0.06504335]), 'std_score_time': array([0.00537936, 0.00238943, 0.00698836]), 'param_min_child_weight': masked_array(data=[1, 5, 10],\n\n             mask=[False, False, False],\n\n       fill_value='?',\n\n            dtype=object), 'params': [{'min_child_weight': 1}, {'min_child_weight': 5}, {'min_child_weight': 10}], 'split0_test_score': array([0.58704137, 0.58456134, 0.58710937]), 'split1_test_score': array([0.59264318, 0.59273012, 0.59195627]), 'split2_test_score': array([0.59217661, 0.59614079, 0.59103669]), 'split3_test_score': array([0.59553925, 0.60373364, 0.59946029]), 'split4_test_score': array([0.5998642 , 0.59430674, 0.59767667]), 'mean_test_score': array([0.59345292, 0.59429452, 0.59344786]), 'std_test_score': array([0.00421544, 0.00615935, 0.00452211]), 'rank_test_score': array([2, 1, 3])}\n\n\n\n Best estimator:\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n\n              colsample_bylevel=None, colsample_bynode=None,\n\n              colsample_bytree=None, early_stopping_rounds=None,\n\n              enable_categorical=False, eval_metric=None, feature_types=None,\n\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n\n              max_cat_threshold=None, max_cat_to_onehot=None,\n\n              max_delta_step=None, max_depth=None, max_leaves=None,\n\n              min_child_weight=5, missing=nan, monotone_constraints=None,\n\n              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n\n              predictor=None, random_state=None, ...)\n\n\n\n Best score:\n\n0.18858904949001554\n\n\n\n Best parameters:\n\n{'min_child_weight': 5}\n"}]},{"cell_type":"markdown","source":"# Make the submission","metadata":{}},{"cell_type":"code","source":"test_pre = pre.transform(test)\ny_test = grid.best_estimator_.predict(test_pre)\ntest['result'] = y_test\ntest['result'].to_csv(\"best_from_grid.csv\",header=False)","metadata":{},"execution_count":15,"outputs":[]}]}